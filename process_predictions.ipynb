{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f59c215a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LosAlamos_NAU-CModel_Flu', 'SigSci-TSENS', 'Flusight-baseline', 'LUcompUncertLab-VAR2K_plusCOVID', 'CU-ensemble', 'JHUAPL-Gecko', 'SGroup-RandomForest', 'VTSanghani-ExogModel', 'SigSci-CREG', 'UMass-trends_ensemble', 'CMU-TimeSeries', 'SGroup-SIkJalpha', 'CEID-Walk', 'LUcompUncertLab-humanjudgment', 'Flusight-ensemble', 'PSI-DICE', 'IEM_Health-FluProject', 'UT_FluCast-Voltaire', 'LUcompUncertLab-VAR2_plusCOVID', 'UVAFluX-Ensemble', 'LUcompUncertLab-VAR2K', 'LUcompUncertLab-TEVA', 'GH-Flusight', 'MOBS-GLEAM_FLUH', 'LUcompUncertLab-VAR2', 'GT-FluFNP-raw', 'GT-FluFNP']\n",
      "9\n",
      "LosAlamos_NAU-CModel_Flu\n",
      "9\n",
      "SigSci-TSENS\n",
      "9\n",
      "Flusight-baseline\n",
      "9\n",
      "LUcompUncertLab-VAR2K_plusCOVID\n",
      "9\n",
      "CU-ensemble\n",
      "6\n",
      "JHUAPL-Gecko\n",
      "5\n",
      "SGroup-RandomForest\n",
      "8\n",
      "VTSanghani-ExogModel\n",
      "9\n",
      "SigSci-CREG\n",
      "7\n",
      "UMass-trends_ensemble\n",
      "9\n",
      "CMU-TimeSeries\n",
      "9\n",
      "SGroup-SIkJalpha\n",
      "7\n",
      "CEID-Walk\n",
      "6\n",
      "LUcompUncertLab-humanjudgment\n",
      "9\n",
      "Flusight-ensemble\n",
      "9\n",
      "PSI-DICE\n",
      "9\n",
      "IEM_Health-FluProject\n",
      "9\n",
      "UT_FluCast-Voltaire\n",
      "9\n",
      "LUcompUncertLab-VAR2_plusCOVID\n",
      "9\n",
      "UVAFluX-Ensemble\n",
      "9\n",
      "LUcompUncertLab-VAR2K\n",
      "9\n",
      "LUcompUncertLab-TEVA\n",
      "6\n",
      "GH-Flusight\n",
      "6\n",
      "MOBS-GLEAM_FLUH\n",
      "8\n",
      "LUcompUncertLab-VAR2\n",
      "9\n",
      "GT-FluFNP-raw\n",
      "9\n",
      "GT-FluFNP\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This is for parsing from CDC's github repo\n",
    "    Output: score card containing all models\n",
    "    i.e., \n",
    "    model, forecast_week, ahead, location (as region abbreviation), type, quantile, value\n",
    "    e.g.\n",
    "    GT-FluFNP, 202205, 1, CA, point, NaN, 843\n",
    "    GT-FluFNP, 202205, 1, CA, quantile, 0.01, 338\n",
    "        ....\n",
    "        GT-FluFNP, 202205, 2, CA, point, NaN, 900\n",
    "        GT-FluFNP, 202205, 2, CA, quantile, 0.01, 438\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import glob\n",
    "from epiweeks import Week\n",
    "import pdb\n",
    "\n",
    "# death_target = ['1 wk ahead inc death' , '2 wk ahead inc death' , '3 wk ahead inc death' , '4 wk ahead inc death']\n",
    "\n",
    "data_ew = Week.thisweek(system=\"CDC\") - 1  # -1 because we have data for the previous (ending) week\n",
    "DIR =  './data-forecasts/'\n",
    "models = [file.split(DIR)[1] for file in glob.glob(DIR + '/*') if \".md\" not in file]\n",
    "location_df = pd.read_csv('data-locations/locations.csv')\n",
    "location_dict = {location_df['location'][i]:location_df['abbreviation'][i] for i in range(len(location_df))}\n",
    "# for each model, get all submissions\n",
    "df_list = []\n",
    "model_point = ['CMU-TimeSeries', 'Flusight-ensemble', 'LUcompUncertLab-TEVA',\n",
    " 'LUcompUncertLab-VAR2', 'LUcompUncertLab-VAR2K', 'LUcompUncertLab-VAR2K_plusCOVID', 'LUcompUncertLab-VAR2_plusCOVID',\n",
    " 'LUcompUncertLab-humanjudgment','LosAlamos_NAU-CModel_Flu','UT_FluCast-Voltaire']\n",
    "print(models)\n",
    "for model in models:\n",
    "    model_dir = DIR + '/' + model + '/' \n",
    "\n",
    "    all_items_path = np.array(glob.glob(model_dir + '*.csv'))  # list all csv files' paths\n",
    "    all_items = [path.replace(model_dir, '') for path in all_items_path]  #list of all csv files' names\n",
    "\n",
    "    \"\"\"\n",
    "    remove forecasts that were duplicated in a given week (if any)\n",
    "    forecasts file should be unique for each epiweek\n",
    "    \"\"\"\n",
    "    subm_dict = {}\n",
    "    for i, item in enumerate(all_items):\n",
    "        date = datetime.strptime(item[:10], '%Y-%m-%d')\n",
    "        epiweek  = date.isocalendar()[1]\n",
    "        if epiweek in subm_dict.keys():\n",
    "            if subm_dict[epiweek][0] <= date:\n",
    "                subm_dict[epiweek] = (date, i)\n",
    "        else:\n",
    "            subm_dict[epiweek] = (date, i)\n",
    "\n",
    "    select = [ value[1] for key, value in subm_dict.items()]\n",
    "    select_paths = all_items_path[select]\n",
    "\n",
    "\n",
    "    data_model = []\n",
    "    for path in select_paths:\n",
    "\n",
    "        df = pd.read_csv(path)\n",
    "        \n",
    "        \"\"\"\n",
    "            create epiweek column\n",
    "        \"\"\"\n",
    "        date = path.split('/')[-1][:10]\n",
    "        # epiweek ends on Saturday, but submission is until Monday. \n",
    "        # we can subtract 2 days, thus, submission on Monday will be considered in the prev week  \n",
    "        # this also aligns submission week and data\n",
    "        date = datetime.strptime(date, '%Y-%m-%d') - timedelta(days=2)\n",
    "        forecast_week = Week.fromdate(date)\n",
    "        df['forecast_week'] = forecast_week\n",
    "        #pdb.set_trace()\n",
    "        data_model.append(df)\n",
    "\n",
    "\n",
    "    # join all dataframes saved in data_model\n",
    "\n",
    "    \"\"\"\n",
    "        select, rename and sort columns\n",
    "    \"\"\"\n",
    "        \n",
    "\n",
    "    \"\"\"\n",
    "        convert location to region abbreviation\n",
    "    \"\"\"\n",
    "    print(len(data_model))\n",
    "    print(model)\n",
    "    df = pd.concat(data_model, ignore_index=True, sort=False)\n",
    "    df = df.rename(columns={'target': 'ahead'})\n",
    "    model_list = []\n",
    "    df['location']= df['location'].astype(str)\n",
    "    for i in range(len(df)):  \n",
    "        key = df['location'][i]\n",
    "        if len(key) == 1: \n",
    "            key = '0' + key\n",
    "        df.at[i, 'location'] = location_dict[key]\n",
    "        df.at[i, 'ahead'] = df['ahead'][i][0]\n",
    "        model_list.append(model)\n",
    "    df['model'] = model\n",
    "    df = df[['model', 'forecast_week', 'ahead', 'location', 'type', 'quantile', 'value']]\n",
    "    final_row = {'model': [], 'forecast_week': [], 'ahead':[], 'location':[],'type':[],'quantile':[],\n",
    "             'value':[]}\n",
    "    for index, row in df.iterrows():\n",
    "        if row['quantile'] == 0.5 and model in model_point: \n",
    "            final_row['model'].append(row['model'])\n",
    "            final_row['forecast_week'].append(row['forecast_week'])\n",
    "            final_row['ahead'].append(row['ahead'])\n",
    "            final_row['location'].append(row['location'])\n",
    "            final_row['type'].append('point')\n",
    "            final_row['quantile'].append(np.nan)\n",
    "            final_row['value'].append(row['value'])\n",
    "    df2 = pd.DataFrame(final_row)\n",
    "    df3 = pd.concat([df,df2], ignore_index = False)\n",
    "    df3 = df3.sort_values(by=['forecast_week', 'location', 'ahead', 'type'], ascending=[True, True,True,True])\n",
    "    df_list.append(df3)  \n",
    "df = pd.concat(df_list, ignore_index=True, sort=False)\n",
    "df = df.sort_values(by=['model','forecast_week', 'location', 'ahead', 'type'], ascending=[True,True, True,True,True])\n",
    "df.to_csv('./predictions.csv',index=False)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2189fc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
